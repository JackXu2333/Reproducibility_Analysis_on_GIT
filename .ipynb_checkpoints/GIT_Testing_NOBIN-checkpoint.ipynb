{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27b88cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\btc2\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2ab3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from base import ModelBase\n",
    "import statistics as sta\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7839505",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6cbd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIT(ModelBase):\n",
    "\n",
    "    def __init__(self, args, **kwargs):\n",
    "\n",
    "        super(GIT, self).__init__(args, **kwargs)\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        self.layer_sga_emb = nn.Embedding(\n",
    "            num_embeddings=self.sga_size+1,\n",
    "            embedding_dim=self.embedding_size,\n",
    "            padding_idx=0)\n",
    "\n",
    "        self.layer_can_emb = nn.Embedding(\n",
    "            num_embeddings=self.can_size+1,\n",
    "            embedding_dim=self.embedding_size,\n",
    "            padding_idx=0)\n",
    "\n",
    "        self.layer_w_0 = nn.Linear(\n",
    "            in_features=self.embedding_size,\n",
    "            out_features=self.attention_size,\n",
    "            bias=True)\n",
    "\n",
    "        self.layer_beta = nn.Linear(\n",
    "            in_features=self.attention_size,\n",
    "            out_features=self.attention_head,\n",
    "            bias=True)\n",
    "\n",
    "        self.layer_dropout_1 = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        self.layer_w_1 = nn.Linear(\n",
    "            in_features=self.embedding_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=True)\n",
    "\n",
    "        self.layer_dropout_2 = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        self.layer_w_2 = nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.deg_size,\n",
    "            bias=True)\n",
    "\n",
    "        if self.initializtion:\n",
    "            gene_emb_pretrain = np.load(os.path.join(self.input_dir, \"gene_emb_pretrain.npy\"))\n",
    "            self.layer_sga_emb.weight.data.copy_(torch.from_numpy(gene_emb_pretrain))\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay)\n",
    "\n",
    "\n",
    "    def forward(self, sga_index, can_index):\n",
    "   \n",
    "        # cancer type embedding\n",
    "        emb_can = self.layer_can_emb(can_index)\n",
    "        emb_can = emb_can.view(-1,self.embedding_size)\n",
    "\n",
    "        # gene embedings\n",
    "        E_t = self.layer_sga_emb(sga_index)\n",
    "\n",
    "        # squeeze and tanh-curve the gene embeddings\n",
    "        E_t_flatten = E_t.view(-1, self.embedding_size)\n",
    "        E_t1_flatten = torch.tanh( self.layer_w_0(E_t_flatten) )\n",
    "\n",
    "        # multiplied by attention heads\n",
    "        E_t2_flatten = self.layer_beta(E_t1_flatten)\n",
    "        E_t2 = E_t2_flatten.view(-1, self.num_max_sga, self.attention_head)\n",
    "\n",
    "        # normalize by softmax\n",
    "        E_t2 = E_t2.permute(1,0,2)\n",
    "        A = F.softmax(E_t2)\n",
    "        A = A.permute(1,0,2)\n",
    "\n",
    "        if self.attention:\n",
    "          # multi-head attention weighted sga embedding:\n",
    "            emb_sga = torch.sum( torch.bmm( A.permute(0,2,1), E_t ), dim=1)\n",
    "            emb_sga = emb_sga.view(-1,self.embedding_size)\n",
    "        else:\n",
    "          # if not using attention, simply sum up SGA embeddings\n",
    "            emb_sga = torch.sum(E_t, dim=1)\n",
    "            emb_sga = emb_sga.view(-1, self.embedding_size)\n",
    "\n",
    "        # if use cancer type input, add cancer type embedding\n",
    "        if self.cancer_type:\n",
    "            emb_tmr = emb_can+emb_sga\n",
    "        else:\n",
    "            emb_tmr = emb_sga\n",
    "\n",
    "        # MLP decoder\n",
    "        emb_tmr_relu = self.layer_dropout_1(emb_tmr)\n",
    "        hid_tmr = self.layer_w_1(emb_tmr_relu)\n",
    "        hid_tmr_relu = self.layer_dropout_2(hid_tmr)\n",
    "        \n",
    "        preds = F.tanh(self.layer_w_2(hid_tmr_relu))\n",
    "\n",
    "        # attention weights\n",
    "        attn_wt = torch.sum(A, dim=2)\n",
    "        attn_wt = attn_wt.view(-1, self.num_max_sga)\n",
    "\n",
    "        return preds, hid_tmr, emb_tmr, emb_sga, attn_wt\n",
    "\n",
    "\n",
    "    def train(self, train_set, test_set,\n",
    "            batch_size=None, test_batch_size=None,\n",
    "            max_iter=None, max_fscore=None,\n",
    "            test_inc_size=None, **kwargs):\n",
    "\n",
    "        for iter_train in range(0, max_iter+1, batch_size):\n",
    "            batch_set = get_minibatch(train_set, iter_train, batch_size,batch_type=\"train\")\n",
    "            preds, _, _, _, _ = self.forward(batch_set[\"sga\"].to(device), batch_set[\"can\"].to(device))\n",
    "            labels = batch_set[\"deg\"].to(device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = -torch.log( self.epsilon + 1 - torch.abs(preds - labels) / 2 ).mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if test_inc_size and (iter_train % test_inc_size == 0):\n",
    "                labels, preds, _, _, _, _, _ = self.test(test_set, test_batch_size)\n",
    "                precision, recall, f1score, accuracy = evaluate(\n",
    "                    labels, preds, epsilon=self.epsilon)\n",
    "                print(\"[%d,%d], precision: %.3f, acc: %.3f\"% (iter_train//len(train_set[\"can\"]),\n",
    "                      iter_train%len(train_set[\"can\"]), precision, accuracy))\n",
    "\n",
    "                if f1score >= max_fscore:\n",
    "                    break\n",
    "\n",
    "        #self.save_model(os.path.join(self.output_dir, \"trained_model.pth\"))\n",
    "\n",
    "\n",
    "    def test(self, test_set, test_batch_size, **kwargs):\n",
    "\n",
    "        labels, preds, hid_tmr, emb_tmr, emb_sga, attn_wt, tmr = [], [], [], [], [], [], []\n",
    "\n",
    "        for iter_test in range(0, len(test_set[\"can\"]), test_batch_size):\n",
    "            batch_set = get_minibatch(test_set, iter_test, test_batch_size, batch_type=\"test\")\n",
    "            batch_preds, batch_hid_tmr, batch_emb_tmr, batch_emb_sga, batch_attn_wt = self.forward(\n",
    "                batch_set[\"sga\"].to(device), batch_set[\"can\"].to(device))\n",
    "            batch_labels = batch_set[\"deg\"].to(device)\n",
    "\n",
    "            labels.append(batch_labels.data.to(torch.device(\"cpu\")).numpy())\n",
    "            preds.append(batch_preds.data.to(torch.device(\"cpu\")).numpy())\n",
    "            hid_tmr.append(batch_hid_tmr.data.to(torch.device(\"cpu\")).numpy())\n",
    "            emb_tmr.append(batch_emb_tmr.data.to(torch.device(\"cpu\")).numpy())\n",
    "            emb_sga.append(batch_emb_sga.data.to(torch.device(\"cpu\")).numpy())\n",
    "            attn_wt.append(batch_attn_wt.data.to(torch.device(\"cpu\")).numpy())\n",
    "            tmr = tmr + batch_set[\"tmr\"]\n",
    "\n",
    "        labels = np.concatenate(labels,axis=0)\n",
    "        preds = np.concatenate(preds,axis=0)\n",
    "        hid_tmr = np.concatenate(hid_tmr,axis=0)\n",
    "        emb_tmr = np.concatenate(emb_tmr,axis=0)\n",
    "        emb_sga = np.concatenate(emb_sga,axis=0)\n",
    "        attn_wt = np.concatenate(attn_wt,axis=0)\n",
    "\n",
    "        return labels, preds, hid_tmr, emb_tmr, emb_sga, attn_wt, tmr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a4dfa",
   "metadata": {},
   "source": [
    "### Non Binary Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1db1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arguments\n",
    "args_nb = SimpleNamespace()\n",
    "\n",
    "args_nb.train_model=True\n",
    "\n",
    "args_nb.input_dir=\"data_noBin\"\n",
    "args_nb.output_dir=\"data_noBin\"\n",
    "\n",
    "args_nb.embedding_size=512\n",
    "args_nb.hidden_size=1024\n",
    "args_nb.attention_size=400\n",
    "args_nb.attention_head=128\n",
    "\n",
    "args_nb.max_fscore=0.7\n",
    "args_nb.batch_size=16\n",
    "args_nb.test_batch_size=512\n",
    "args_nb.test_inc_size=256\n",
    "args_nb.dropout_rate=0.5\n",
    "args_nb.weight_decay=1e-5\n",
    "\n",
    "args_nb.deg_shuffle=False\n",
    "args_nb.nonbinary=True\n",
    "\n",
    "# Load data\n",
    "dataset_nb = load_dataset(input_dir=args_nb.input_dir, deg_shuffle=args_nb.deg_shuffle)\n",
    "train_set_nb, test_set_nb = split_dataset(dataset_nb, ratio=0.66)\n",
    "\n",
    "args_nb.can_size = dataset_nb[\"can\"].max()        # cancer type dimension\n",
    "args_nb.sga_size = max(dataset_nb[\"sga\"].max(), 19781)        # SGA dimension\n",
    "args_nb.deg_size = dataset_nb[\"deg\"].shape[1]     # DEG output dimension\n",
    "args_nb.num_max_sga = dataset_nb[\"sga\"].shape[1]  # maximum number of SGAs in a tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e72c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\btc2\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,0], precision: 0.168, acc: 0.347\n",
      "[0,256], precision: 0.236, acc: 0.393\n",
      "[0,512], precision: 0.282, acc: 0.411\n",
      "[0,768], precision: 0.306, acc: 0.417\n",
      "[0,1024], precision: 0.315, acc: 0.420\n",
      "[0,1280], precision: 0.319, acc: 0.428\n",
      "[0,1536], precision: 0.322, acc: 0.443\n",
      "[0,1792], precision: 0.325, acc: 0.460\n",
      "[0,2048], precision: 0.328, acc: 0.480\n",
      "[0,2304], precision: 0.333, acc: 0.498\n",
      "[0,2560], precision: 0.339, acc: 0.515\n",
      "[0,2816], precision: 0.346, acc: 0.532\n",
      "[1,124], precision: 0.353, acc: 0.547\n",
      "[1,380], precision: 0.362, acc: 0.558\n",
      "[1,636], precision: 0.370, acc: 0.567\n",
      "[1,892], precision: 0.377, acc: 0.574\n",
      "[1,1148], precision: 0.384, acc: 0.578\n",
      "[1,1404], precision: 0.389, acc: 0.585\n",
      "[1,1660], precision: 0.395, acc: 0.593\n",
      "[1,1916], precision: 0.400, acc: 0.599\n",
      "[1,2172], precision: 0.405, acc: 0.602\n",
      "[1,2428], precision: 0.408, acc: 0.605\n",
      "[1,2684], precision: 0.414, acc: 0.609\n",
      "[1,2940], precision: 0.419, acc: 0.617\n",
      "[2,248], precision: 0.421, acc: 0.619\n",
      "[2,504], precision: 0.425, acc: 0.621\n",
      "[2,760], precision: 0.428, acc: 0.625\n",
      "[2,1016], precision: 0.431, acc: 0.628\n",
      "[2,1272], precision: 0.433, acc: 0.629\n",
      "[2,1528], precision: 0.438, acc: 0.634\n",
      "[2,1784], precision: 0.440, acc: 0.637\n",
      "[2,2040], precision: 0.443, acc: 0.639\n",
      "[2,2296], precision: 0.446, acc: 0.643\n",
      "[2,2552], precision: 0.449, acc: 0.644\n",
      "[2,2808], precision: 0.452, acc: 0.647\n",
      "[3,116], precision: 0.453, acc: 0.648\n",
      "[3,372], precision: 0.456, acc: 0.651\n",
      "[3,628], precision: 0.460, acc: 0.655\n",
      "[3,884], precision: 0.464, acc: 0.658\n",
      "[3,1140], precision: 0.464, acc: 0.658\n",
      "[3,1396], precision: 0.468, acc: 0.662\n",
      "[3,1652], precision: 0.472, acc: 0.666\n",
      "[3,1908], precision: 0.474, acc: 0.667\n",
      "[3,2164], precision: 0.475, acc: 0.668\n",
      "[3,2420], precision: 0.476, acc: 0.668\n",
      "[3,2676], precision: 0.481, acc: 0.673\n",
      "[3,2932], precision: 0.484, acc: 0.675\n",
      "[4,240], precision: 0.484, acc: 0.675\n",
      "[4,496], precision: 0.488, acc: 0.679\n",
      "[4,752], precision: 0.493, acc: 0.682\n",
      "[4,1008], precision: 0.493, acc: 0.683\n",
      "[4,1264], precision: 0.493, acc: 0.683\n",
      "[4,1520], precision: 0.499, acc: 0.687\n",
      "[4,1776], precision: 0.502, acc: 0.690\n",
      "[4,2032], precision: 0.502, acc: 0.690\n",
      "[4,2288], precision: 0.505, acc: 0.692\n",
      "[4,2544], precision: 0.508, acc: 0.694\n",
      "[4,2800], precision: 0.513, acc: 0.698\n",
      "[5,108], precision: 0.515, acc: 0.699\n",
      "[5,364], precision: 0.514, acc: 0.698\n",
      "[5,620], precision: 0.518, acc: 0.701\n",
      "[5,876], precision: 0.523, acc: 0.704\n",
      "[5,1132], precision: 0.523, acc: 0.705\n",
      "[5,1388], precision: 0.528, acc: 0.707\n",
      "[5,1644], precision: 0.532, acc: 0.710\n",
      "[5,1900], precision: 0.532, acc: 0.710\n",
      "[5,2156], precision: 0.534, acc: 0.712\n",
      "[5,2412], precision: 0.535, acc: 0.712\n",
      "[5,2668], precision: 0.539, acc: 0.715\n",
      "[5,2924], precision: 0.542, acc: 0.717\n",
      "[6,232], precision: 0.541, acc: 0.716\n",
      "[6,488], precision: 0.546, acc: 0.719\n",
      "[6,744], precision: 0.551, acc: 0.722\n",
      "[6,1000], precision: 0.549, acc: 0.721\n",
      "[6,1256], precision: 0.549, acc: 0.721\n",
      "[6,1512], precision: 0.556, acc: 0.725\n",
      "[6,1768], precision: 0.560, acc: 0.727\n",
      "[6,2024], precision: 0.559, acc: 0.727\n",
      "[6,2280], precision: 0.563, acc: 0.728\n",
      "[6,2536], precision: 0.564, acc: 0.730\n",
      "[6,2792], precision: 0.567, acc: 0.731\n",
      "[7,100], precision: 0.572, acc: 0.733\n",
      "[7,356], precision: 0.571, acc: 0.733\n",
      "[7,612], precision: 0.572, acc: 0.734\n",
      "[7,868], precision: 0.580, acc: 0.737\n",
      "[7,1124], precision: 0.577, acc: 0.736\n",
      "[7,1380], precision: 0.580, acc: 0.738\n",
      "[7,1636], precision: 0.585, acc: 0.740\n",
      "[7,1892], precision: 0.584, acc: 0.740\n",
      "[7,2148], precision: 0.583, acc: 0.740\n",
      "[7,2404], precision: 0.584, acc: 0.740\n",
      "[7,2660], precision: 0.592, acc: 0.744\n",
      "[7,2916], precision: 0.593, acc: 0.744\n",
      "[8,224], precision: 0.591, acc: 0.744\n",
      "[8,480], precision: 0.595, acc: 0.746\n",
      "[8,736], precision: 0.601, acc: 0.748\n",
      "[8,992], precision: 0.602, acc: 0.748\n",
      "[8,1248], precision: 0.599, acc: 0.747\n",
      "[8,1504], precision: 0.607, acc: 0.750\n",
      "[8,1760], precision: 0.606, acc: 0.750\n",
      "[8,2016], precision: 0.607, acc: 0.751\n",
      "[8,2272], precision: 0.604, acc: 0.749\n",
      "[8,2528], precision: 0.604, acc: 0.750\n",
      "[8,2784], precision: 0.613, acc: 0.753\n",
      "[9,92], precision: 0.615, acc: 0.754\n",
      "[9,348], precision: 0.612, acc: 0.753\n",
      "[9,604], precision: 0.616, acc: 0.755\n",
      "[9,860], precision: 0.624, acc: 0.757\n",
      "[9,1116], precision: 0.616, acc: 0.755\n",
      "[9,1372], precision: 0.620, acc: 0.756\n",
      "[9,1628], precision: 0.626, acc: 0.758\n",
      "[9,1884], precision: 0.627, acc: 0.759\n",
      "[9,2140], precision: 0.628, acc: 0.760\n",
      "[9,2396], precision: 0.623, acc: 0.758\n",
      "[9,2652], precision: 0.631, acc: 0.761\n",
      "[9,2908], precision: 0.631, acc: 0.761\n",
      "[10,216], precision: 0.627, acc: 0.760\n",
      "[10,472], precision: 0.636, acc: 0.762\n",
      "[10,728], precision: 0.640, acc: 0.764\n",
      "[10,984], precision: 0.633, acc: 0.761\n",
      "[10,1240], precision: 0.631, acc: 0.762\n",
      "[10,1496], precision: 0.645, acc: 0.765\n",
      "[10,1752], precision: 0.643, acc: 0.765\n",
      "[10,2008], precision: 0.645, acc: 0.765\n",
      "[10,2264], precision: 0.642, acc: 0.765\n",
      "[10,2520], precision: 0.643, acc: 0.766\n",
      "[10,2776], precision: 0.648, acc: 0.767\n",
      "[11,84], precision: 0.651, acc: 0.767\n",
      "[11,340], precision: 0.646, acc: 0.766\n",
      "[11,596], precision: 0.649, acc: 0.767\n",
      "[11,852], precision: 0.654, acc: 0.768\n",
      "[11,1108], precision: 0.647, acc: 0.767\n",
      "[11,1364], precision: 0.651, acc: 0.768\n",
      "[11,1620], precision: 0.657, acc: 0.769\n",
      "[11,1876], precision: 0.654, acc: 0.769\n",
      "[11,2132], precision: 0.655, acc: 0.769\n",
      "[11,2388], precision: 0.652, acc: 0.768\n",
      "[11,2644], precision: 0.655, acc: 0.770\n",
      "[11,2900], precision: 0.663, acc: 0.771\n",
      "[12,208], precision: 0.655, acc: 0.770\n",
      "[12,464], precision: 0.657, acc: 0.771\n",
      "[12,720], precision: 0.666, acc: 0.772\n",
      "[12,976], precision: 0.661, acc: 0.771\n",
      "[12,1232], precision: 0.659, acc: 0.771\n",
      "[12,1488], precision: 0.665, acc: 0.772\n",
      "[12,1744], precision: 0.669, acc: 0.773\n",
      "[12,2000], precision: 0.666, acc: 0.772\n",
      "[12,2256], precision: 0.665, acc: 0.772\n",
      "[12,2512], precision: 0.660, acc: 0.771\n",
      "[12,2768], precision: 0.665, acc: 0.773\n",
      "[13,76], precision: 0.672, acc: 0.774\n",
      "[13,332], precision: 0.666, acc: 0.773\n",
      "[13,588], precision: 0.670, acc: 0.774\n",
      "[13,844], precision: 0.669, acc: 0.774\n",
      "[13,1100], precision: 0.666, acc: 0.773\n",
      "[13,1356], precision: 0.669, acc: 0.774\n",
      "[13,1612], precision: 0.673, acc: 0.774\n",
      "[13,1868], precision: 0.674, acc: 0.775\n",
      "[13,2124], precision: 0.675, acc: 0.775\n",
      "[13,2380], precision: 0.667, acc: 0.773\n",
      "[13,2636], precision: 0.671, acc: 0.775\n",
      "[13,2892], precision: 0.677, acc: 0.776\n",
      "[14,200], precision: 0.678, acc: 0.776\n",
      "[14,456], precision: 0.678, acc: 0.776\n",
      "[14,712], precision: 0.682, acc: 0.777\n",
      "[14,968], precision: 0.677, acc: 0.776\n",
      "[14,1224], precision: 0.674, acc: 0.775\n",
      "[14,1480], precision: 0.682, acc: 0.777\n",
      "[14,1736], precision: 0.686, acc: 0.777\n",
      "[14,1992], precision: 0.678, acc: 0.776\n",
      "[14,2248], precision: 0.682, acc: 0.777\n",
      "[14,2504], precision: 0.680, acc: 0.777\n",
      "[14,2760], precision: 0.680, acc: 0.777\n",
      "[15,68], precision: 0.680, acc: 0.777\n",
      "[15,324], precision: 0.682, acc: 0.778\n",
      "[15,580], precision: 0.683, acc: 0.777\n",
      "[15,836], precision: 0.687, acc: 0.778\n",
      "[15,1092], precision: 0.680, acc: 0.777\n",
      "[15,1348], precision: 0.683, acc: 0.778\n",
      "[15,1604], precision: 0.689, acc: 0.779\n",
      "[15,1860], precision: 0.687, acc: 0.778\n",
      "[15,2116], precision: 0.687, acc: 0.779\n",
      "[15,2372], precision: 0.683, acc: 0.778\n",
      "[15,2628], precision: 0.686, acc: 0.779\n",
      "[15,2884], precision: 0.685, acc: 0.778\n",
      "[16,192], precision: 0.684, acc: 0.778\n",
      "[16,448], precision: 0.686, acc: 0.779\n",
      "[16,704], precision: 0.692, acc: 0.779\n",
      "[16,960], precision: 0.691, acc: 0.779\n",
      "[16,1216], precision: 0.685, acc: 0.779\n",
      "[16,1472], precision: 0.692, acc: 0.780\n",
      "[16,1728], precision: 0.694, acc: 0.780\n",
      "[16,1984], precision: 0.687, acc: 0.779\n",
      "[16,2240], precision: 0.693, acc: 0.780\n",
      "[16,2496], precision: 0.687, acc: 0.779\n",
      "[16,2752], precision: 0.689, acc: 0.780\n",
      "[17,60], precision: 0.691, acc: 0.780\n",
      "[17,316], precision: 0.688, acc: 0.779\n",
      "[17,572], precision: 0.690, acc: 0.779\n",
      "[17,828], precision: 0.697, acc: 0.781\n",
      "[17,1084], precision: 0.692, acc: 0.780\n",
      "[17,1340], precision: 0.692, acc: 0.780\n",
      "[17,1596], precision: 0.699, acc: 0.781\n",
      "[17,1852], precision: 0.698, acc: 0.781\n",
      "[17,2108], precision: 0.695, acc: 0.780\n",
      "[17,2364], precision: 0.694, acc: 0.781\n",
      "[17,2620], precision: 0.689, acc: 0.780\n",
      "[17,2876], precision: 0.695, acc: 0.781\n",
      "[18,184], precision: 0.692, acc: 0.780\n",
      "[18,440], precision: 0.693, acc: 0.780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,696], precision: 0.697, acc: 0.781\n",
      "[18,952], precision: 0.691, acc: 0.780\n",
      "[18,1208], precision: 0.692, acc: 0.780\n",
      "[18,1464], precision: 0.696, acc: 0.780\n",
      "[18,1720], precision: 0.699, acc: 0.781\n",
      "[18,1976], precision: 0.698, acc: 0.781\n",
      "[18,2232], precision: 0.696, acc: 0.781\n",
      "[18,2488], precision: 0.690, acc: 0.780\n",
      "[18,2744], precision: 0.696, acc: 0.781\n",
      "[19,52], precision: 0.698, acc: 0.781\n",
      "[19,308], precision: 0.690, acc: 0.780\n",
      "[19,564], precision: 0.699, acc: 0.782\n",
      "[19,820], precision: 0.699, acc: 0.781\n",
      "[19,1076], precision: 0.695, acc: 0.781\n"
     ]
    }
   ],
   "source": [
    "precision_nb_GIT, recall_nb_GIT, f1score_nb_GIT, accuracy_nb_GIT = [], [], [], []\n",
    "\n",
    "# GIT variants:\n",
    "# args.initializtion=False -> GIT-init\n",
    "args_nb.initializtion=True\n",
    "# args.attention=False -> GIT-attn\n",
    "args_nb.attention=True\n",
    "# args.cancer_type=False -> GIT-can\n",
    "args_nb.cancer_type=True\n",
    "\n",
    "args_nb.max_iter=3072*20\n",
    "args_nb.learning_rate=1e-4\n",
    "\n",
    "if args_nb.cancer_type == False:\n",
    "    args_nb.max_iter = 3072*40\n",
    "elif args_nb.attention == False:\n",
    "    args_nb.max_iter = 3072*40\n",
    "    args_nb.learning_rate = 0.0003\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    # Init model with single hidden layer\n",
    "    model = GIT(args_nb).to(device)\n",
    "\n",
    "    # Train MLP model\n",
    "    model.train(train_set_nb, test_set_nb,\n",
    "          batch_size=args_nb.batch_size,\n",
    "          test_batch_size=args_nb.test_batch_size,\n",
    "          max_iter=args_nb.max_iter,\n",
    "          max_fscore=args_nb.max_fscore,\n",
    "          test_inc_size=args_nb.test_inc_size)\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    labels, preds, _, _, _, _, _ = model.test(test_set_nb, test_batch_size=512)\n",
    "    precision, recall, f1score, accuracy = evaluate(labels, preds, epsilon=1e-4)\n",
    "    print(\"prec=%.3f, recall=%.3f, F1=%.3f, acc=%.3f\"%(precision, recall, f1score, accuracy))\n",
    "    \n",
    "    # Release memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    precision_nb_GIT.append(precision)\n",
    "    recall_nb_GIT.append(recall)\n",
    "    f1score_nb_GIT.append(f1score)\n",
    "    accuracy_nb_GIT.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ab885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prec=%.3f, recall=%.3f, F1=%.3f, acc=%.3f\"%\n",
    "      (sta.mean(precision_nb_GIT), sta.mean(recall_nb_GIT), sta.mean(f1score_nb_GIT), sta.mean(accuracy_nb_GIT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prec=0.696, recall=0.537, F1=0.606, acc=0.781\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bdd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3d532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d0997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9c9897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0., -0., -0., ..., -0., -0.,  1.],\n",
       "       [ 1., -0., -0., ..., -0., -1., -0.],\n",
       "       [-0., -0.,  1., ..., -0., -1.,  1.],\n",
       "       ...,\n",
       "       [-1., -0., -1., ..., -0., -0., -0.],\n",
       "       [-0.,  0.,  0., ...,  1.,  0., -0.],\n",
       "       [ 0.,  1.,  0., ...,  0., -0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b91c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
